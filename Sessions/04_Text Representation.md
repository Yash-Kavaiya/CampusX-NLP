# Text Representation | NLP Lecture 4 | Bag of Words | Tf-Idf | N-grams, Bi-grams and Uni-grams

# Feature Extraction in NLP: Text Representation ğŸ“ŠğŸ”¤

## 1. What is Feature Extraction (Text Representation)? ğŸ¤”

**Feature extraction** in NLP is the process of transforming raw text data into numerical vectors that machine learning algorithms can understand and process. It's essentially a **translation mechanism** that converts human language into a mathematical format.

```
Raw Text â†’ Feature Extraction â†’ Numerical Representation
"I love NLP" â†’ [Feature Extractor] â†’ [0.2, 0.8, 0.3, ...]
```

> ğŸ’¡ **Key Insight**: This transformation bridges the gap between human language and computational processing.

## 2. Why Do We Need Feature Extraction? ğŸ¯

### Primary Reasons:

| Reason | Explanation |
|--------|-------------|
| **Algorithm Compatibility** | Machine learning algorithms require numerical inputs, not raw text |
| **Dimensionality Management** | Reduces the infinite space of language to manageable dimensions |
| **Pattern Recognition** | Enables algorithms to detect statistical patterns in language |
| **Computational Efficiency** | Makes processing large text corpora feasible |

### Without feature extraction:
```
"I love NLP" â†’ ??? â†’ ML Algorithm
                  â†‘
                (No way to process)
```

## 3. Why is Feature Extraction Difficult? âš ï¸

### Challenges in Text Representation:

- **Language Complexity** ğŸ“
  - Ambiguity: "The bank was robbed" (financial institution or riverbank?)
  - Polysemy: Words with multiple meanings
  - Homonyms: Same spelling, different meanings

- **Contextual Meaning** ğŸ”„
  - "It's cold" means different things in different contexts

- **Structural Information** ğŸ—ï¸
  - Word order matters: "Dog bites man" â‰  "Man bites dog"
  - Syntactic relationships are crucial

- **Semantic Richness** ğŸŒ
  - Capturing nuance, sentiment, and implied meanings

## 4. What is the Core Idea? ğŸ’¡

The fundamental concept is **embedding** - mapping words or texts to points in a vector space where:

1. **Similar meanings â†’ Similar vectors**
2. **Semantic relationships â†’ Geometric relationships**
3. **Linguistic properties â†’ Mathematical properties**

```
                Vector Space
                    â†‘
"king" â†’ [0.2, 0.6, ...] 
                    â†“ 
    Similar position to semantically similar words
```

## 5. What are the Techniques? ğŸ› ï¸

### A. Traditional Methods

#### 1ï¸âƒ£ **One-Hot Encoding**
```python
# Vocabulary: ["I", "love", "NLP", "machine", "learning"]
# "I love NLP" â†’ [1, 1, 1, 0, 0]
```
- âœ… Simple
- âŒ No semantic information
- âŒ Sparse, high-dimensional

#### 2ï¸âƒ£ **Bag-of-Words (BoW)**
```python
# "I love NLP" â†’ {"I": 1, "love": 1, "NLP": 1}
# "I love machine learning" â†’ {"I": 1, "love": 1, "machine": 1, "learning": 1}
```
- âœ… Simple frequency counting
- âŒ Loses word order
- âŒ No semantics

#### 3ï¸âƒ£ **TF-IDF (Term Frequency-Inverse Document Frequency)**
```
TF-IDF(t, d) = TF(t, d) Ã— IDF(t)
```
- âœ… Weights terms by importance
- âœ… Reduces impact of common words
- âŒ Still ignores word order

#### 4ï¸âƒ£ **N-grams**
```
"I love NLP"
1-grams: ["I", "love", "NLP"]
2-grams: ["I love", "love NLP"]
```
- âœ… Captures some word order
- âŒ Sparsity increases exponentially

### B. Modern Embedding Techniques

#### 1ï¸âƒ£ **Word Embeddings**

**Word2Vec**
```
"king" - "man" + "woman" â‰ˆ "queen"
```
- âœ… Dense vectors
- âœ… Captures semantic relationships
- âŒ Single representation per word

**GloVe (Global Vectors)**
- âœ… Combines global statistics with local context
- âœ… Pre-trained on large corpora

**FastText**
- âœ… Handles out-of-vocabulary words
- âœ… Works with subword information

#### 2ï¸âƒ£ **Contextual Embeddings**

**ELMo (Embeddings from Language Models)**
- âœ… Different vectors for same word in different contexts
- âœ… Bi-directional language model

**BERT (Bidirectional Encoder Representations from Transformers)**
```
"bank" in "river bank" â‰  "bank" in "financial bank"
```
- âœ… Deeply bidirectional
- âœ… Captures complex contextual relationships
- âœ… Pre-trained on massive corpora

**Transformer-based Models**
- GPT, RoBERTa, XLNet, T5
- âœ… State-of-the-art performance
- âœ… Self-attention mechanisms

## Summary: The Evolution of Text Representation ğŸ“ˆ

```
One-Hot â†’ Bag-of-Words â†’ TF-IDF â†’ Word2Vec â†’ BERT
   â†“            â†“            â†“         â†“         â†“
Simple      Frequency     Weighted   Semantic   Contextual
```

### Key Takeaways:
- Feature extraction translates text into machine-readable format
- Techniques range from simple counting to complex neural models
- Modern methods preserve semantics, context, and relationships
- The field continues to evolve with transformer architectures

---

ğŸ” **Further Reading**: Word embeddings, Transformer architecture, Transfer learning in NLP

# ğŸ“š Common Terms in NLP ğŸ”¤

## 1. Corpus ğŸ“Š

A **corpus** is a large, structured collection of texts used for linguistic analysis and model training.

| ğŸ“‹ Key Characteristics | ğŸ“ Description |
|------------------------|----------------|
| ğŸ”¢ **Size** | Can range from thousands to billions of documents |
| ğŸŒ **Domain** | May be general (web text) or specialized (medical literature) |
| ğŸ”„ **Format** | Usually stored in plain text or specialized formats |
| ğŸ“ **Quality** | Can be raw or cleaned/preprocessed |

> ğŸ’¡ **Practical Example**: *The Brown Corpus* was one of the first major electronic corpora, containing ~1 million words of American English texts published in 1961.

### Common Types of Corpora:
- ğŸŒ **General corpora**: Broad language samples (Wikipedia dumps, Common Crawl)
- ğŸ¯ **Domain-specific corpora**: Focused on particular fields (PubMed, legal documents)
- ğŸ“£ **Parallel corpora**: Same content in multiple languages (Europarl)
- ğŸ—£ï¸ **Speech corpora**: Spoken language samples (LibriSpeech)

## 2. Vocabulary ğŸ“”

A **vocabulary** (or lexicon) is the complete set of unique tokens (usually words) that appear in a corpus.

```mermaid
graph TD
    A[Raw Corpus] --> B[Tokenization]
    B --> C[Unique Tokens]
    C --> D[Vocabulary]
    D --> E[Indexed Word List]
    E --> F[Word-to-Index Mapping]
```

| ğŸ“Š Vocabulary Considerations | ğŸ” Impact |
|------------------------------|----------|
| ğŸ“ˆ **Size** | Larger vocabulary = more memory requirements, potential sparsity issues |
| ğŸ§® **Frequency cutoffs** | Removing rare words reduces vocabulary size |
| ğŸ”  **Case sensitivity** | "The" vs. "the" â€“ separate entries or normalized? |
| âœ‚ï¸ **Subword units** | Using pieces of words (WordPiece, BPE) for OOV handling |

> ğŸ’¡ **Key Insight**: Vocabulary choice directly impacts feature extraction â€“ it defines the dimensions of your vector space.

## 3. Document ğŸ“„

A **document** is a single text unit in a corpus, which can vary widely in length and structure.

| ğŸ“‘ Document Types | ğŸ“ Scale | ğŸ” Examples |
|-------------------|----------|------------|
| ğŸ”¹ **Micro** | Very small | Tweet, search query, product title |
| ğŸ”¸ **Small** | Paragraph-sized | News headline, abstract, short comment |
| ğŸ“ **Medium** | Article-sized | News article, essay, email, review |
| ğŸ“š **Large** | Extended content | Research paper, book chapter, transcript |

### Document Representation:
- ğŸ“Š **Bag-of-words**: Document as word frequency vector
- ğŸ§© **Sequence**: Document as ordered tokens
- ğŸŒ **Graph**: Document as connected entity/concept network
- ğŸ”¢ **Dense vector**: Document as fixed-length embedding

## 4. Word ğŸ”¤

A **word** refers to a basic unit of language that carries meaning, though the definition becomes complex in computational contexts.

| ğŸ“ Related Concept | ğŸ“‹ Definition | ğŸ” Example |
|-------------------|--------------|------------|
| ğŸ§© **Token** | Unit after text segmentation | "don't" â†’ ["don", "'", "t"] |
| ğŸŒ± **Stem** | Word with suffix removed | "running" â†’ "run" |
| ğŸ“– **Lemma** | Dictionary form of a word | "better" â†’ "good" |
| ğŸ”¡ **Wordpiece** | Subword unit | "uncommon" â†’ ["un", "common"] |

### Word Complexity Challenges:
- ğŸ”€ **Compounds**: "ice cream" â€“ one concept, two tokens?
- ğŸŒ‰ **Multi-word expressions**: "New York City" â€“ single entity
- â– **Hyphenation**: "state-of-the-art" â€“ one word or multiple?
- ğŸ”¤ **Case sensitivity**: "apple" vs. "Apple" (fruit vs. company)
- ğŸŒ **Cross-lingual considerations**: Words in languages without clear word boundaries

---

## ğŸ”„ Relationships Between Terms

```mermaid
graph TD
    A[Corpus] -->|contains many| B[Documents]
    B -->|contains many| C[Words]
    A -->|defines| D[Vocabulary]
    D -->|indexes all unique| C
```

# ğŸ”¢ One-Hot Encoding in NLP ğŸ“Š

## What is One-Hot Encoding? ğŸ¯

**One-hot encoding** is a representation technique that transforms categorical data into a binary vector format where:
- Each categorical value becomes a vector of 0s
- Exactly one position in the vector contains a 1 (hence "one-hot")
- The vector length equals the number of categories

> ğŸ’¡ **Core Concept**: One-hot encoding creates a mathematical representation of words or documents that machines can process, sacrificing efficiency for simplicity and interpretability.

## ğŸ“‹ Your Example Corpus Analysis

```
D1: people watch campus
D2: campus watch campus
D3: people write comment
D4: campus write comment

Vocabulary (V=5): [people, watch, campus, write, comment]
```

## ğŸ”¤ One-Hot Encoding of Words

In word-level one-hot encoding, each word is represented as a vector where only one position is "hot" (set to 1).

| Word | Vector Representation |
|------|----------------------|
| ğŸ§‘ **people** | [1, 0, 0, 0, 0] |
| ğŸ‘ï¸ **watch** | [0, 1, 0, 0, 0] |
| ğŸ« **campus** | [0, 0, 1, 0, 0] |
| âœï¸ **write** | [0, 0, 0, 1, 0] |
| ğŸ’¬ **comment** | [0, 0, 0, 0, 1] |

### ğŸ“Š Visual Representation

```mermaid
graph TD
    A["Word: 'campus'"] --> B["[0, 0, 1, 0, 0]"]
    C["Position in Vocabulary"] --> D["[people, watch, campus, write, comment]"]
    E["'Hot' Position"] --> F["[0, 0, 1, 0, 0]"]
```

## ğŸ“„ One-Hot Encoding of Documents

### Binary Document Vectors (Presence/Absence)

When representing documents, one-hot encoding commonly uses a **binary bag-of-words** approach:

| Document | Content | Binary Vector |
|----------|---------|--------------|
| ğŸ“‘ **D1** | "people watch campus" | [1, 1, 1, 0, 0] |
| ğŸ“‘ **D2** | "campus watch campus" | [0, 1, 1, 0, 0] |
| ğŸ“‘ **D3** | "people write comment" | [1, 0, 0, 1, 1] |
| ğŸ“‘ **D4** | "campus write comment" | [0, 0, 1, 1, 1] |

### Count-Based Document Vectors (Frequency)

A more informative representation counts word occurrences:

| Document | people | watch | campus | write | comment |
|----------|--------|-------|--------|-------|---------|
| ğŸ“‘ **D1** | 1 | 1 | 1 | 0 | 0 |
| ğŸ“‘ **D2** | 0 | 1 | 2 | 0 | 0 |
| ğŸ“‘ **D3** | 1 | 0 | 0 | 1 | 1 |
| ğŸ“‘ **D4** | 0 | 0 | 1 | 1 | 1 |

> ğŸ” **Note**: In D2, "campus" appears twice, so its count is 2 rather than 1 in the frequency-based representation.

## âš™ï¸ Implementation Process

```mermaid
graph TD
    A[Text Documents] --> B[Tokenization]
    B --> C[Build Vocabulary]
    C --> D[Create Vector Space]
    D --> E[Encode Words/Documents]
    E --> F[Sparse Matrices]
```

## ğŸ” Mathematical View

For a vocabulary of size V, word w<sub>i</sub> is represented as a vector x<sub>i</sub> âˆˆ â„<sup>V</sup> where:

$$x_i[j] = \begin{cases}
1 & \text{if } j = i \\
0 & \text{otherwise}
\end{cases}$$

## âœ… Advantages and âŒ Limitations

| Advantages | Limitations |
|------------|-------------|
| ğŸŸ¢ Simple to implement | ğŸ”´ High dimensionality (V-sized vectors) |
| ğŸŸ¢ No assumptions about relationships | ğŸ”´ Sparse representation (mostly zeros) |
| ğŸŸ¢ Equal distance between all words | ğŸ”´ No semantic information captured |
| ğŸŸ¢ Interpretable dimensions | ğŸ”´ Cannot handle out-of-vocabulary words |
| ğŸŸ¢ Works with any ML algorithm | ğŸ”´ Ignores word order and context |

## ğŸ”„ Relationship to Other Text Representations

One-hot encoding serves as the foundation for more advanced representations:

- **TF-IDF**: Weighted version of document vectors
- **Co-occurrence matrices**: Built from one-hot vectors
- **Word embeddings**: Dense vectors that address sparsity issues
- **Topic models**: Use document-term matrices derived from one-hot representations

## ğŸ§® Sample Calculations with Your Corpus

### Document Similarity Using One-Hot Vectors

Let's compute similarity between documents using the binary representation:

```
Similarity(D1, D2) = Dot product([1,1,1,0,0], [0,1,1,0,0]) = 2
Similarity(D1, D3) = Dot product([1,1,1,0,0], [1,0,0,1,1]) = 1
```

> ğŸ’¡ This simple analysis shows D1 and D2 share more common terms (2) than D1 and D3 (1).


