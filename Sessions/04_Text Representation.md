# Text Representation | NLP Lecture 4 | Bag of Words | Tf-Idf | N-grams, Bi-grams and Uni-grams

# Feature Extraction in NLP: Text Representation ğŸ“ŠğŸ”¤

## 1. What is Feature Extraction (Text Representation)? ğŸ¤”

**Feature extraction** in NLP is the process of transforming raw text data into numerical vectors that machine learning algorithms can understand and process. It's essentially a **translation mechanism** that converts human language into a mathematical format.

```
Raw Text â†’ Feature Extraction â†’ Numerical Representation
"I love NLP" â†’ [Feature Extractor] â†’ [0.2, 0.8, 0.3, ...]
```

> ğŸ’¡ **Key Insight**: This transformation bridges the gap between human language and computational processing.

## 2. Why Do We Need Feature Extraction? ğŸ¯

### Primary Reasons:

| Reason | Explanation |
|--------|-------------|
| **Algorithm Compatibility** | Machine learning algorithms require numerical inputs, not raw text |
| **Dimensionality Management** | Reduces the infinite space of language to manageable dimensions |
| **Pattern Recognition** | Enables algorithms to detect statistical patterns in language |
| **Computational Efficiency** | Makes processing large text corpora feasible |

### Without feature extraction:
```
"I love NLP" â†’ ??? â†’ ML Algorithm
                  â†‘
                (No way to process)
```

## 3. Why is Feature Extraction Difficult? âš ï¸

### Challenges in Text Representation:

- **Language Complexity** ğŸ“
  - Ambiguity: "The bank was robbed" (financial institution or riverbank?)
  - Polysemy: Words with multiple meanings
  - Homonyms: Same spelling, different meanings

- **Contextual Meaning** ğŸ”„
  - "It's cold" means different things in different contexts

- **Structural Information** ğŸ—ï¸
  - Word order matters: "Dog bites man" â‰  "Man bites dog"
  - Syntactic relationships are crucial

- **Semantic Richness** ğŸŒ
  - Capturing nuance, sentiment, and implied meanings

## 4. What is the Core Idea? ğŸ’¡

The fundamental concept is **embedding** - mapping words or texts to points in a vector space where:

1. **Similar meanings â†’ Similar vectors**
2. **Semantic relationships â†’ Geometric relationships**
3. **Linguistic properties â†’ Mathematical properties**

```
                Vector Space
                    â†‘
"king" â†’ [0.2, 0.6, ...] 
                    â†“ 
    Similar position to semantically similar words
```

## 5. What are the Techniques? ğŸ› ï¸

### A. Traditional Methods

#### 1ï¸âƒ£ **One-Hot Encoding**
```python
# Vocabulary: ["I", "love", "NLP", "machine", "learning"]
# "I love NLP" â†’ [1, 1, 1, 0, 0]
```
- âœ… Simple
- âŒ No semantic information
- âŒ Sparse, high-dimensional

#### 2ï¸âƒ£ **Bag-of-Words (BoW)**
```python
# "I love NLP" â†’ {"I": 1, "love": 1, "NLP": 1}
# "I love machine learning" â†’ {"I": 1, "love": 1, "machine": 1, "learning": 1}
```
- âœ… Simple frequency counting
- âŒ Loses word order
- âŒ No semantics

#### 3ï¸âƒ£ **TF-IDF (Term Frequency-Inverse Document Frequency)**
```
TF-IDF(t, d) = TF(t, d) Ã— IDF(t)
```
- âœ… Weights terms by importance
- âœ… Reduces impact of common words
- âŒ Still ignores word order

#### 4ï¸âƒ£ **N-grams**
```
"I love NLP"
1-grams: ["I", "love", "NLP"]
2-grams: ["I love", "love NLP"]
```
- âœ… Captures some word order
- âŒ Sparsity increases exponentially

### B. Modern Embedding Techniques

#### 1ï¸âƒ£ **Word Embeddings**

**Word2Vec**
```
"king" - "man" + "woman" â‰ˆ "queen"
```
- âœ… Dense vectors
- âœ… Captures semantic relationships
- âŒ Single representation per word

**GloVe (Global Vectors)**
- âœ… Combines global statistics with local context
- âœ… Pre-trained on large corpora

**FastText**
- âœ… Handles out-of-vocabulary words
- âœ… Works with subword information

#### 2ï¸âƒ£ **Contextual Embeddings**

**ELMo (Embeddings from Language Models)**
- âœ… Different vectors for same word in different contexts
- âœ… Bi-directional language model

**BERT (Bidirectional Encoder Representations from Transformers)**
```
"bank" in "river bank" â‰  "bank" in "financial bank"
```
- âœ… Deeply bidirectional
- âœ… Captures complex contextual relationships
- âœ… Pre-trained on massive corpora

**Transformer-based Models**
- GPT, RoBERTa, XLNet, T5
- âœ… State-of-the-art performance
- âœ… Self-attention mechanisms

## Summary: The Evolution of Text Representation ğŸ“ˆ

```
One-Hot â†’ Bag-of-Words â†’ TF-IDF â†’ Word2Vec â†’ BERT
   â†“            â†“            â†“         â†“         â†“
Simple      Frequency     Weighted   Semantic   Contextual
```

### Key Takeaways:
- Feature extraction translates text into machine-readable format
- Techniques range from simple counting to complex neural models
- Modern methods preserve semantics, context, and relationships
- The field continues to evolve with transformer architectures

---

ğŸ” **Further Reading**: Word embeddings, Transformer architecture, Transfer learning in NLP

# ğŸ“š Common Terms in NLP ğŸ”¤

## 1. Corpus ğŸ“Š

A **corpus** is a large, structured collection of texts used for linguistic analysis and model training.

| ğŸ“‹ Key Characteristics | ğŸ“ Description |
|------------------------|----------------|
| ğŸ”¢ **Size** | Can range from thousands to billions of documents |
| ğŸŒ **Domain** | May be general (web text) or specialized (medical literature) |
| ğŸ”„ **Format** | Usually stored in plain text or specialized formats |
| ğŸ“ **Quality** | Can be raw or cleaned/preprocessed |

> ğŸ’¡ **Practical Example**: *The Brown Corpus* was one of the first major electronic corpora, containing ~1 million words of American English texts published in 1961.

### Common Types of Corpora:
- ğŸŒ **General corpora**: Broad language samples (Wikipedia dumps, Common Crawl)
- ğŸ¯ **Domain-specific corpora**: Focused on particular fields (PubMed, legal documents)
- ğŸ“£ **Parallel corpora**: Same content in multiple languages (Europarl)
- ğŸ—£ï¸ **Speech corpora**: Spoken language samples (LibriSpeech)

## 2. Vocabulary ğŸ“”

A **vocabulary** (or lexicon) is the complete set of unique tokens (usually words) that appear in a corpus.

```mermaid
graph TD
    A[Raw Corpus] --> B[Tokenization]
    B --> C[Unique Tokens]
    C --> D[Vocabulary]
    D --> E[Indexed Word List]
    E --> F[Word-to-Index Mapping]
```

| ğŸ“Š Vocabulary Considerations | ğŸ” Impact |
|------------------------------|----------|
| ğŸ“ˆ **Size** | Larger vocabulary = more memory requirements, potential sparsity issues |
| ğŸ§® **Frequency cutoffs** | Removing rare words reduces vocabulary size |
| ğŸ”  **Case sensitivity** | "The" vs. "the" â€“ separate entries or normalized? |
| âœ‚ï¸ **Subword units** | Using pieces of words (WordPiece, BPE) for OOV handling |

> ğŸ’¡ **Key Insight**: Vocabulary choice directly impacts feature extraction â€“ it defines the dimensions of your vector space.

## 3. Document ğŸ“„

A **document** is a single text unit in a corpus, which can vary widely in length and structure.

| ğŸ“‘ Document Types | ğŸ“ Scale | ğŸ” Examples |
|-------------------|----------|------------|
| ğŸ”¹ **Micro** | Very small | Tweet, search query, product title |
| ğŸ”¸ **Small** | Paragraph-sized | News headline, abstract, short comment |
| ğŸ“ **Medium** | Article-sized | News article, essay, email, review |
| ğŸ“š **Large** | Extended content | Research paper, book chapter, transcript |

### Document Representation:
- ğŸ“Š **Bag-of-words**: Document as word frequency vector
- ğŸ§© **Sequence**: Document as ordered tokens
- ğŸŒ **Graph**: Document as connected entity/concept network
- ğŸ”¢ **Dense vector**: Document as fixed-length embedding

## 4. Word ğŸ”¤

A **word** refers to a basic unit of language that carries meaning, though the definition becomes complex in computational contexts.

| ğŸ“ Related Concept | ğŸ“‹ Definition | ğŸ” Example |
|-------------------|--------------|------------|
| ğŸ§© **Token** | Unit after text segmentation | "don't" â†’ ["don", "'", "t"] |
| ğŸŒ± **Stem** | Word with suffix removed | "running" â†’ "run" |
| ğŸ“– **Lemma** | Dictionary form of a word | "better" â†’ "good" |
| ğŸ”¡ **Wordpiece** | Subword unit | "uncommon" â†’ ["un", "common"] |

### Word Complexity Challenges:
- ğŸ”€ **Compounds**: "ice cream" â€“ one concept, two tokens?
- ğŸŒ‰ **Multi-word expressions**: "New York City" â€“ single entity
- â– **Hyphenation**: "state-of-the-art" â€“ one word or multiple?
- ğŸ”¤ **Case sensitivity**: "apple" vs. "Apple" (fruit vs. company)
- ğŸŒ **Cross-lingual considerations**: Words in languages without clear word boundaries

---

## ğŸ”„ Relationships Between Terms

```mermaid
graph TD
    A[Corpus] -->|contains many| B[Documents]
    B -->|contains many| C[Words]
    A -->|defines| D[Vocabulary]
    D -->|indexes all unique| C
```

# ğŸ”¢ One-Hot Encoding in NLP ğŸ“Š

## What is One-Hot Encoding? ğŸ¯

**One-hot encoding** is a representation technique that transforms categorical data into a binary vector format where:
- Each categorical value becomes a vector of 0s
- Exactly one position in the vector contains a 1 (hence "one-hot")
- The vector length equals the number of categories

> ğŸ’¡ **Core Concept**: One-hot encoding creates a mathematical representation of words or documents that machines can process, sacrificing efficiency for simplicity and interpretability.

## ğŸ“‹ Your Example Corpus Analysis

```
D1: people watch campus
D2: campus watch campus
D3: people write comment
D4: campus write comment

Vocabulary (V=5): [people, watch, campus, write, comment]
```

## ğŸ”¤ One-Hot Encoding of Words

In word-level one-hot encoding, each word is represented as a vector where only one position is "hot" (set to 1).

| Word | Vector Representation |
|------|----------------------|
| ğŸ§‘ **people** | [1, 0, 0, 0, 0] |
| ğŸ‘ï¸ **watch** | [0, 1, 0, 0, 0] |
| ğŸ« **campus** | [0, 0, 1, 0, 0] |
| âœï¸ **write** | [0, 0, 0, 1, 0] |
| ğŸ’¬ **comment** | [0, 0, 0, 0, 1] |

### ğŸ“Š Visual Representation

```mermaid
graph TD
    A["Word: 'campus'"] --> B["[0, 0, 1, 0, 0]"]
    C["Position in Vocabulary"] --> D["[people, watch, campus, write, comment]"]
    E["'Hot' Position"] --> F["[0, 0, 1, 0, 0]"]
```

## ğŸ“„ One-Hot Encoding of Documents

### Binary Document Vectors (Presence/Absence)

When representing documents, one-hot encoding commonly uses a **binary bag-of-words** approach:

| Document | Content | Binary Vector |
|----------|---------|--------------|
| ğŸ“‘ **D1** | "people watch campus" | [1, 1, 1, 0, 0] |
| ğŸ“‘ **D2** | "campus watch campus" | [0, 1, 1, 0, 0] |
| ğŸ“‘ **D3** | "people write comment" | [1, 0, 0, 1, 1] |
| ğŸ“‘ **D4** | "campus write comment" | [0, 0, 1, 1, 1] |

### Count-Based Document Vectors (Frequency)

A more informative representation counts word occurrences:

| Document | people | watch | campus | write | comment |
|----------|--------|-------|--------|-------|---------|
| ğŸ“‘ **D1** | 1 | 1 | 1 | 0 | 0 |
| ğŸ“‘ **D2** | 0 | 1 | 2 | 0 | 0 |
| ğŸ“‘ **D3** | 1 | 0 | 0 | 1 | 1 |
| ğŸ“‘ **D4** | 0 | 0 | 1 | 1 | 1 |

> ğŸ” **Note**: In D2, "campus" appears twice, so its count is 2 rather than 1 in the frequency-based representation.

## âš™ï¸ Implementation Process

```mermaid
graph TD
    A[Text Documents] --> B[Tokenization]
    B --> C[Build Vocabulary]
    C --> D[Create Vector Space]
    D --> E[Encode Words/Documents]
    E --> F[Sparse Matrices]
```

## ğŸ” Mathematical View

For a vocabulary of size V, word w<sub>i</sub> is represented as a vector x<sub>i</sub> âˆˆ â„<sup>V</sup> where:

$$x_i[j] = \begin{cases}
1 & \text{if } j = i \\
0 & \text{otherwise}
\end{cases}$$

## âœ… Advantages and âŒ Limitations

| Advantages | Limitations |
|------------|-------------|
| ğŸŸ¢ Simple to implement | ğŸ”´ High dimensionality (V-sized vectors) |
| ğŸŸ¢ No assumptions about relationships | ğŸ”´ Sparse representation (mostly zeros) |
| ğŸŸ¢ Equal distance between all words | ğŸ”´ No semantic information captured |
| ğŸŸ¢ Interpretable dimensions | ğŸ”´ Cannot handle out-of-vocabulary words |
| ğŸŸ¢ Works with any ML algorithm | ğŸ”´ Ignores word order and context |

## ğŸ”„ Relationship to Other Text Representations

One-hot encoding serves as the foundation for more advanced representations:

- **TF-IDF**: Weighted version of document vectors
- **Co-occurrence matrices**: Built from one-hot vectors
- **Word embeddings**: Dense vectors that address sparsity issues
- **Topic models**: Use document-term matrices derived from one-hot representations

## ğŸ§® Sample Calculations with Your Corpus

### Document Similarity Using One-Hot Vectors

Let's compute similarity between documents using the binary representation:

```
Similarity(D1, D2) = Dot product([1,1,1,0,0], [0,1,1,0,0]) = 2
Similarity(D1, D3) = Dot product([1,1,1,0,0], [1,0,0,1,1]) = 1
```

> ğŸ’¡ This simple analysis shows D1 and D2 share more common terms (2) than D1 and D3 (1).
# ğŸ“Š N-grams in NLP: Beyond Single Words ğŸ”¤

## ğŸ” What Are N-grams?

**N-grams** are contiguous sequences of n items (words, characters, or tokens) from a text document. Unlike the standard Bag of Words model that treats each word individually, N-grams capture adjacent elements, preserving some of the sequential information in text.

> ğŸ’¡ **Key Insight**: N-grams attempt to preserve local word order patterns that single-word models miss entirely.

```mermaid
graph LR
    A[Text] --> B[Tokenization]
    B --> C[Generate Sequences]
    C --> D[Count N-gram Frequencies]
    D --> E[Create Feature Vectors]
```

## ğŸ§© Types of N-grams

| N-gram Type | Description | Example from "people watch campusx" |
|-------------|-------------|-----------------------------------|
| ğŸ”¹ **Unigrams** (n=1) | Single tokens | "people", "watch", "campusx" |
| ğŸ”¹ **Bigrams** (n=2) | Pairs of adjacent tokens | "people watch", "watch campusx" |
| ğŸ”¹ **Trigrams** (n=3) | Triplets of adjacent tokens | "people watch campusx" |
| ğŸ”¹ **4-grams** (n=4) | Four adjacent tokens | (none in this short example) |

## ğŸ¯ Bigram Example from the Image

Your image shows how documents are represented using bigrams:

### ğŸ“‹ Document Corpus
- D1: "people watch campusx"
- D2: "campusx watch campusx"
- D3: "people write comment"
- D4: "campusx write comment"

### ğŸ“Š Bigram Vocabulary (V=8)
1. "people watch"
2. "watch campusx"
3. "campusx watch"
4. "people write"
5. "write comment"
6. "campusx write"

### ğŸ“ˆ Bigram Document-Term Matrix

| Document | people watch | watch campusx | campusx watch | people write | write comment | campusx write |
|----------|--------------|---------------|---------------|--------------|---------------|---------------|
| D1       | 1            | 1             | 0             | 0            | 0             | 0             |
| D2       | 0            | 1             | 1             | 0            | 0             | 0             |
| D3       | 0            | 0             | 0             | 1            | 1             | 0             |
| D4       | 0            | 0             | 0             | 0            | 1             | 1             |

## âš™ï¸ Implementation Process

```mermaid
graph TD
    A[Text Documents] --> B[Tokenization]
    B --> C[Generate N-grams]
    C --> D[Build N-gram Vocabulary]
    D --> E[Count N-gram Frequencies]
    E --> F[Create Feature Vectors]
```

## ğŸ“ Mathematical Representation

For a text with m tokens [wâ‚, wâ‚‚, ..., wâ‚˜], the set of n-grams is:

$$\{(w_i, w_{i+1}, ..., w_{i+n-1}) \mid 1 \leq i \leq m-n+1\}$$

## âœ… Advantages of N-grams

| Advantage | Description | Impact |
|-----------|-------------|--------|
| ğŸŒŸ **Preserves Local Context** | Captures word order and phrases | Better semantic preservation than BOW |
| ğŸŒŸ **Language Model Foundation** | Basis for probabilistic language models | Enables text generation and prediction |
| ğŸŒŸ **Simple Implementation** | Straightforward extension of BOW | Easily integrated into existing pipelines |
| ğŸŒŸ **No Training Required** | Direct counting from corpus | Fast to implement compared to neural models |
| ğŸŒŸ **Handles Fixed Expressions** | Better representation of idioms, phrases | "New York" stays together as a concept |
| ğŸŒŸ **Improved Classification** | Often increases accuracy in text classification | Better features for ML algorithms |

## âŒ Disadvantages of N-grams

| Disadvantage | Description | Impact |
|--------------|-------------|--------|
| ğŸ”´ **Exponential Vocabulary Growth** | Vocabulary size grows dramatically with n | Memory and computation costs increase |
| ğŸ”´ **Data Sparsity** | Most n-grams appear very rarely | Sparse vectors with many zeros |
| ğŸ”´ **Limited Context Window** | Only captures local patterns within n words | Misses long-range dependencies |
| ğŸ”´ **Out-of-Vocabulary Problem** | Any unseen n-gram is completely missed | Poor generalization to new text |
| ğŸ”´ **Rigid Sequence Matching** | Requires exact matches (no synonyms) | "very good" â‰  "really good" |
| ğŸ”´ **No Semantic Understanding** | Still fundamentally a counting model | Misses deeper meaning |

## ğŸ” N-gram Smoothing Techniques

To address the sparsity problem, several smoothing methods exist:

| Technique | Description | Benefit |
|-----------|-------------|---------|
| ğŸ› ï¸ **Laplace (Add-1)** | Add 1 to all counts | Simple but often overestimates rare events |
| ğŸ› ï¸ **Add-k** | Add k (fraction) to all counts | More flexible than Add-1 |
| ğŸ› ï¸ **Good-Turing** | Reallocate probability mass | Better estimates for rare events |
| ğŸ› ï¸ **Kneser-Ney** | Uses absolute discounting | State-of-the-art for n-gram models |
| ğŸ› ï¸ **Backoff Models** | Fall back to (n-1)-grams when n-gram unseen | More robust predictions |

## ğŸ“Š Comparing N-gram Models

| Model | Context Captured | Vector Space Size | Sparsity | Common Applications |
|-------|------------------|-------------------|----------|---------------------|
| ğŸ”¹ **Unigrams** | None | V | Low | Topic classification, basic IR |
| ğŸ”¹ **Bigrams** | 1 previous word | VÂ² (potential) | Medium | Phrase detection, basic context |
| ğŸ”¹ **Trigrams** | 2 previous words | VÂ³ (potential) | High | Language modeling, speech recognition |
| ğŸ”¹ **4-grams+** | 3+ previous words | V^n (potential) | Very High | Specialized language modeling |

## ğŸš€ Applications of N-grams

| Application | How N-grams Are Used | Why It Works |
|-------------|----------------------|-------------|
| ğŸ“ **Language Identification** | Character n-grams frequencies | Languages have distinct n-gram patterns |
| ğŸ”¤ **Spelling Correction** | Character n-gram probability | Likely character sequences guide corrections |
| ğŸ¯ **Text Classification** | Word n-gram features | Phrases provide better classification signals |
| ğŸ” **Information Retrieval** | Query-document n-gram matching | Multi-word queries match better |
| ğŸ“Š **Language Modeling** | Predict next word from previous n-1 words | Statistical patterns in language |
| ğŸ§  **Machine Translation** | Phrase-based translation | Better than word-by-word translation |

## ğŸ§® Character N-grams vs. Word N-grams

| Type | Advantages | Common Uses |
|------|------------|------------|
| ğŸ”¤ **Character N-grams** | â€¢ Handles misspellings<br>â€¢ Smaller vocabulary<br>â€¢ Works across languages | â€¢ Language ID<br>â€¢ Authorship analysis<br>â€¢ Spam detection |
| ğŸ“š **Word N-grams** | â€¢ Captures meaningful phrases<br>â€¢ Better semantic representation<br>â€¢ More intuitive | â€¢ Topic modeling<br>â€¢ Sentiment analysis<br>â€¢ Text generation |

## ğŸŒ N-grams in Modern NLP

While deep learning approaches have surpassed n-grams in performance for many NLP tasks, n-grams still remain relevant:

- ğŸ”¹ As features in hybrid systems
- ğŸ”¹ In resource-constrained environments
- ğŸ”¹ For interpretable models where feature importance matters
- ğŸ”¹ As baselines for comparing more complex models

> ğŸ’¡ **Evolution of Context Capture**: N-grams â†’ RNNs â†’ Attention â†’ Transformers

## ğŸ§© Code Example: Generating N-grams

```python
def generate_ngrams(text, n):
    # Tokenize the text
    tokens = text.lower().split()
    
    # Generate n-grams
    ngrams = []
    for i in range(len(tokens) - n + 1):
        ngram = ' '.join(tokens[i:i+n])
        ngrams.append(ngram)
    
    return ngrams

# Example
text = "people watch campusx"
print(generate_ngrams(text, 2))  # Outputs: ['people watch', 'watch campusx']
```

## ğŸŒŸ The Big Picture: N-grams as Feature Extraction

N-grams represent a crucial bridge between the simplicity of Bag-of-Words and more sophisticated context-aware models. While they cannot capture long-range dependencies or true semantic understanding, their ability to preserve local word order makes them a significant improvement over single-word models.

---

N-grams balance the trade-off between model complexity and contextual awareness, providing a robust framework that remains relevant even in today's deep learning-dominated NLP landscape.

# ğŸ“Š TF-IDF: Term Frequency-Inverse Document Frequency ğŸ“š

## ğŸ” What is TF-IDF?

**TF-IDF** (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects the importance of a word to a document in a collection of documents (corpus). It's one of the most powerful feature extraction techniques in NLP.

> ğŸ’¡ **Core Insight**: Not all words are created equal. TF-IDF increases the weight of terms that are frequent in a document but rare across the corpus, helping identify distinctive terms.

```mermaid
graph LR
    A[Raw Text] --> B[Term Frequency]
    A --> C[Inverse Document Frequency]
    B --> D[TF-IDF Score]
    C --> D
    D --> E[Document Vectors]
```

## ğŸ§® Mathematical Formulation

TF-IDF consists of two components multiplied together:

$$\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)$$

Where:
- t = term (word)
- d = document
- D = collection of documents (corpus)

### ğŸ“ˆ Term Frequency (TF)

TF measures how frequently a term occurs in a document:

$$\text{TF}(t, d) = \frac{\text{Number of times term t appears in document d}}{\text{Total number of terms in document d}}$$

### ğŸ“‰ Inverse Document Frequency (IDF)

IDF measures how important a term is across the entire corpus:

$$\text{IDF}(t, D) = \log\left(\frac{\text{Total number of documents in corpus}}{\text{Number of documents containing term t}}\right)$$

> ğŸ”‘ **Key Point**: The logarithm dampens the effect of IDF for very rare terms.

## âš™ï¸ Calculating TF-IDF: Step-by-Step Process

```mermaid
graph TD
    A[Input Documents] --> B[Tokenize Documents]
    B --> C[Calculate TF for each term in each document]
    B --> D[Calculate IDF for each term in corpus]
    C --> E[Multiply TF and IDF]
    D --> E
    E --> F[Create TF-IDF Matrix]
```

### 1ï¸âƒ£ Calculate Term Frequency (TF)

For each term in each document:

| TF Calculation Methods | Formula | Notes |
|------------------------|---------|-------|
| ğŸ”¹ **Raw Count** | TF(t,d) = f(t,d) | Simple count of term occurrences |
| ğŸ”¹ **Boolean Frequency** | TF(t,d) = 1 if t occurs in d, 0 otherwise | Just indicates presence |
| ğŸ”¹ **Term Frequency** | TF(t,d) = f(t,d) / âˆ‘f(t',d) | Normalized by document length |
| ğŸ”¹ **Log Normalization** | TF(t,d) = 1 + log(f(t,d)) | Dampens effect of high-frequency terms |
| ğŸ”¹ **Double Normalization** | TF(t,d) = 0.5 + 0.5 * f(t,d) / max_t'(f(t',d)) | Scales between 0.5 and 1 |

### 2ï¸âƒ£ Calculate Inverse Document Frequency (IDF)

For each term across all documents:

| IDF Calculation Methods | Formula | Notes |
|-------------------------|---------|-------|
| ğŸ”¹ **Standard IDF** | IDF(t) = log(N / df(t)) | N = total docs, df = docs containing t |
| ğŸ”¹ **Smooth IDF** | IDF(t) = log(1 + N / df(t)) | Prevents division by zero |
| ğŸ”¹ **Probabilistic IDF** | IDF(t) = log((N - df(t)) / df(t)) | Derived from probabilistic model |

### 3ï¸âƒ£ Calculate TF-IDF

For each term in each document, multiply TF and IDF.

## ğŸ“Š Practical Example

Let's work through a simple example using our corpus from previous examples:

| Document | Content |
|----------|---------|
| ğŸ“‘ **D1** | "people watch campusx" |
| ğŸ“‘ **D2** | "campusx watch campusx" |
| ğŸ“‘ **D3** | "people write comment" |
| ğŸ“‘ **D4** | "campusx write comment" |

### Step 1: Calculate Term Frequency (TF)

| Term | TF in D1 | TF in D2 | TF in D3 | TF in D4 |
|------|----------|----------|----------|----------|
| people | 1/3 = 0.33 | 0 | 1/3 = 0.33 | 0 |
| watch | 1/3 = 0.33 | 1/3 = 0.33 | 0 | 0 |
| campusx | 1/3 = 0.33 | 2/3 = 0.67 | 0 | 1/3 = 0.33 |
| write | 0 | 0 | 1/3 = 0.33 | 1/3 = 0.33 |
| comment | 0 | 0 | 1/3 = 0.33 | 1/3 = 0.33 |

### Step 2: Calculate Document Frequency (DF) and IDF

| Term | DF | IDF = log(4/DF) |
|------|----|--------------------|
| people | 2 | log(4/2) = log(2) â‰ˆ 0.301 |
| watch | 2 | log(4/2) = log(2) â‰ˆ 0.301 |
| campusx | 3 | log(4/3) â‰ˆ 0.125 |
| write | 2 | log(4/2) = log(2) â‰ˆ 0.301 |
| comment | 2 | log(4/2) = log(2) â‰ˆ 0.301 |

### Step 3: Calculate TF-IDF

| Term | TF-IDF in D1 | TF-IDF in D2 | TF-IDF in D3 | TF-IDF in D4 |
|------|--------------|--------------|--------------|--------------|
| people | 0.33 Ã— 0.301 â‰ˆ 0.099 | 0 | 0.33 Ã— 0.301 â‰ˆ 0.099 | 0 |
| watch | 0.33 Ã— 0.301 â‰ˆ 0.099 | 0.33 Ã— 0.301 â‰ˆ 0.099 | 0 | 0 |
| campusx | 0.33 Ã— 0.125 â‰ˆ 0.041 | 0.67 Ã— 0.125 â‰ˆ 0.084 | 0 | 0.33 Ã— 0.125 â‰ˆ 0.041 |
| write | 0 | 0 | 0.33 Ã— 0.301 â‰ˆ 0.099 | 0.33 Ã— 0.301 â‰ˆ 0.099 |
| comment | 0 | 0 | 0.33 Ã— 0.301 â‰ˆ 0.099 | 0.33 Ã— 0.301 â‰ˆ 0.099 |

### Resulting TF-IDF Document Vectors

```
D1: [0.099, 0.099, 0.041, 0, 0]
D2: [0, 0.099, 0.084, 0, 0]
D3: [0.099, 0, 0, 0.099, 0.099]
D4: [0, 0, 0.041, 0.099, 0.099]
```

## ğŸ” Interpreting TF-IDF Values

| Value Range | Interpretation | Example |
|-------------|----------------|---------|
| ğŸ”¼ **High TF-IDF** | Term is important and distinctive to this document | "campusx" in D2 has highest value (appears multiple times but not in all docs) |
| ğŸ”½ **Low TF-IDF** | Term is either rare in the document or common across documents | Common terms or rarely used terms |
| 0ï¸âƒ£ **Zero TF-IDF** | Term doesn't appear in the document | "write" in D1 and D2 |

## âœ… Advantages of TF-IDF

| Advantage | Description | Impact |
|-----------|-------------|--------|
| ğŸŒŸ **Importance Weighting** | Distinguishes between important and common terms | Better document representation |
| ğŸŒŸ **Noise Reduction** | Reduces impact of frequently occurring but less informative words | Cleaner feature space |
| ğŸŒŸ **Simple Yet Effective** | Computationally efficient with good results | Industry standard for decades |
| ğŸŒŸ **No Training Required** | Direct calculation from corpus statistics | Can be applied immediately to any dataset |
| ğŸŒŸ **Interpretable** | Values have clear meaning | Useful for feature analysis and selection |
| ğŸŒŸ **Domain Adaptability** | Automatically adjusts to domain-specific terminology | Works across various text types |

## âŒ Disadvantages of TF-IDF

| Disadvantage | Description | Impact |
|--------------|-------------|--------|
| ğŸ”´ **Ignores Semantics** | Doesn't capture meaning or relationships between words | Semantic similarity not represented |
| ğŸ”´ **Ignores Word Order** | Bag-of-words approach loses sequential information | Context is lost |
| ğŸ”´ **High Dimensionality** | Feature space grows with vocabulary size | Curse of dimensionality issues |
| ğŸ”´ **Sparse Vectors** | Most entries are zero | Computational efficiency challenges |
| ğŸ”´ **Limited Context** | No consideration of document or corpus structure | Misses hierarchical information |
| ğŸ”´ **Requires Preprocessing** | Sensitive to stemming, stopword removal choices | Results depend on preprocessing steps |

## ğŸš€ Common Applications of TF-IDF

```mermaid
graph TD
    A[TF-IDF] --> B[Information Retrieval]
    A --> C[Document Classification]
    A --> D[Document Clustering]
    A --> E[Keyword Extraction]
    A --> F[Text Summarization]
    A --> G[Recommendation Systems]
```

| Application | How TF-IDF Is Used | Why It's Effective |
|-------------|-------------------|-------------------|
| ğŸ” **Search Engines** | Ranking documents by query term importance | Prioritizes distinctive matches over common term matches |
| ğŸ“Š **Topic Modeling** | Identifying distinctive terms for topics | Highlights discriminative features |
| ğŸ§® **Document Similarity** | Computing cosine similarity between TF-IDF vectors | Better than raw word count similarity |
| ğŸ“‘ **Text Summarization** | Identifying sentences with high TF-IDF words | Captures key content |
| ğŸ·ï¸ **Keyword Extraction** | Words with highest TF-IDF scores become keywords | Identifies distinctive terms |

## ğŸŒ Extensions and Variations

| Variation | Description | Advantage Over Basic TF-IDF |
|-----------|-------------|----------------------------|
| ğŸ“Š **BM25** (Okapi BM25) | Probabilistic extension with length normalization | Better handles long documents |
| ğŸ“ˆ **Pivoted Length Normalization** | Adjusts for document length bias | More fair comparison across different document lengths |
| ğŸ§  **LSI/LSA** | Applies SVD to TF-IDF matrix | Captures latent semantic structure |
| ğŸ§® **Delta TF-IDF** | Compares TF-IDF across different corpora | Identifies distinctive terms between collections |
| ğŸ”„ **TF-PDF** | Term Frequency-Proportional Document Frequency | Alternative to IDF for short texts |

## ğŸ’» Implementation Example (Python)

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Our corpus
corpus = [
    "people watch campusx",
    "campusx watch campusx",
    "people write comment",
    "campusx write comment"
]

# Create TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

# Transform documents to TF-IDF features
tfidf_matrix = vectorizer.fit_transform(corpus)

# Get feature names
feature_names = vectorizer.get_feature_names_out()

# Print results
for i, doc in enumerate(corpus):
    print(f"Document {i+1}: {doc}")
    # Get non-zero features for this document
    feature_index = tfidf_matrix[i,:].nonzero()[1]
    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])
    # Sort by score
    for idx, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True):
        print(f"  {feature_names[idx]}: {score:.6f}")
```

## ğŸ”„ TF-IDF vs. Other Text Representation Methods

| Method | Captures Word Importance | Captures Semantics | Dimensionality | Training Required |
|--------|-------------------------|-------------------|---------------|-------------------|
| ğŸ“Š **Bag of Words** | âŒ | âŒ | High (V) | âŒ |
| ğŸ“ˆ **TF-IDF** | âœ… | âŒ | High (V) | âŒ |
| ğŸ”¤ **Word Embeddings** | âŒ | âœ… | Low (300-500) | âœ… |
| ğŸ§  **Topic Models** | âœ… | âœ… | Low (topics) | âœ… |
| ğŸ”„ **Transformers** | âœ… | âœ… | Medium | âœ… |

## ğŸŒŸ Evolution in NLP: From TF-IDF to Modern Approaches

```mermaid
graph LR
    A[Bag of Words] --> B[TF-IDF]
    B --> C[LSA/LSI]
    C --> D[Word Embeddings]
    D --> E[Contextual Embeddings]
    E --> F[Transformer Models]
```

Despite its limitations, TF-IDF remains a foundational technique in NLP. Its computational efficiency, interpretability, and effectiveness make it a valuable tool for many text analysis tasks, even in the era of deep learning and transformer models.

> ğŸ’¡ **Final Insight**: TF-IDF strikes an elegant balance between simplicity and effectiveness. While newer methods can capture more semantic nuance, TF-IDF's combination of local (TF) and global (IDF) statistics provides a remarkably powerful representation with minimal computational cost.

